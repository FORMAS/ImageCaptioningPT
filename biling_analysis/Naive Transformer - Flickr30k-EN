{"cells":[{"cell_type":"markdown","source":["**Acknowledgments**: This code was mainly based on the tutorial by Aakash Kumar Nain, available [here](https://keras.io/examples/vision/image_captioning/), to whom we extend our thanks for their contribution and for making the material publicly available."],"metadata":{"id":"lwSaeLbfRiDa"}},{"cell_type":"markdown","metadata":{"id":"42x0bbTh_qsc"},"source":["## Setup\n","### 6 heads 2 layers enc-dec EN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTiNJAZqHt84"},"outputs":[],"source":["pip install tensorflow==2.8.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEfKshQD_qsd"},"outputs":[],"source":["import os, json\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import ast\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications import EfficientNetV2L\n","from tensorflow.keras.layers import TextVectorization\n","\n","\n","seed = 111\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107518,"status":"ok","timestamp":1680705071003,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"ZkcCSjXWAH1B","outputId":"d5f78100-32ae-40c2-c1a3-12fbcb095fda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BipKr2a4_qse"},"outputs":[],"source":["with open('/content/drive/MyDrive/Mestrado/ImageCaptioning/datasets/flickr30k/dataset_flickr30k_allEN.json', 'r') as captionsJson:\n","  captions = json.load(captionsJson)\n","with open('/content/drive/MyDrive/Mestrado/ImageCaptioning/datasets/flickr30k/dataset_flickr30k.json', 'r') as captionsJson:\n","  info = json.load(captionsJson)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lh_oJH-rAYJe"},"outputs":[],"source":["!tar -xf /content/drive/MyDrive/Mestrado/ImageCaptioning/datasets/flickr30k/flickr30k-images.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPrBgwDT_qse"},"outputs":[],"source":["\n","# Path to the images\n","IMAGES_PATH = \"flickr30k-images\"\n","\n","# Desired image dimensions\n","IMAGE_SIZE = (384, 384)\n","\n","# Fixed length allowed for any sequence\n","SEQ_LENGTH = 25\n","\n","# Other training parameters\n","BATCH_SIZE = 64\n","AUTOTUNE = tf.data.AUTOTUNE"]},{"cell_type":"markdown","metadata":{"id":"1enfCX9q_qsf"},"source":["## Preparing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19725,"status":"ok","timestamp":1680705167297,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"paYGaujC_qsg","outputId":"92b4979d-33fd-42bd-fd1f-f821b7863ec2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training samples:  29000\n","Number of validation samples:  1014\n","Number of test samples:  1000\n"]}],"source":["\n","def load_captions_data(captionsfile):\n","    \"\"\"Loads captions (text) data and maps them to corresponding images.\n","\n","    Args:\n","        filename: Path to the text file containing caption data.\n","\n","    Returns:\n","        caption_mapping: Dictionary mapping image names and the corresponding captions\n","        text_data: List containing all the available captions\n","    \"\"\"\n","    caption_mapping = {}\n","    text_data = []\n","    images_to_skip = set()\n","\n","    for img_name, captions in captionsfile.items():\n","        img_name = os.path.join(IMAGES_PATH, img_name)\n","        for caption in captions:\n","          tokens = caption.strip().split()\n","\n","          if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n","              # We will add a start and an end token to each caption\n","              text_data.append(caption)\n","\n","              if img_name in caption_mapping:\n","                  caption_mapping[img_name].append(caption)\n","              else:\n","                  caption_mapping[img_name] = [caption]\n","\n","    return caption_mapping, text_data\n","\n","\n","def train_val_split(caption_data, train_n, val_n, test_n, shuffle=True):\n","    \"\"\"Split the captioning dataset into train and validation sets.\n","\n","    Args:\n","        caption_data (dict): Dictionary containing the mapped caption data\n","        train_size (float): Fraction of all the full dataset to use as training data\n","        shuffle (bool): Whether to shuffle the dataset before splitting\n","\n","    Returns:\n","        Traning and validation datasets as two separated dicts\n","    \"\"\"\n","\n","    # 1. Get the list of all image names\n","    all_images = list(caption_data.keys())\n","\n","    # 2. Shuffle if necessary\n","    if shuffle:\n","        np.random.shuffle(all_images)\n","\n","    # 3. Split into training and validation sets\n","    train_size=train_n\n","    val_size=val_n\n","    test_size=test_n\n","\n","    training_data = {\n","        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n","    }\n","    validation_data = {\n","        img_name: caption_data[img_name] for img_name in all_images[train_size:train_size+val_size]\n","    }\n","    test_data = {\n","        img_name: caption_data[img_name] for img_name in all_images[-test_size:]\n","    }\n","\n","    # 4. Return the splits\n","    return training_data, validation_data, test_data\n","\n","def train_val_split_karpathy(caption_data, info_data):\n","    \"\"\"Split the captioning dataset into train and validation sets.\n","\n","    Args:\n","        caption_data (dict): Dictionary containing the mapped caption data\n","        train_size (float): Fraction of all the full dataset to use as training data\n","        shuffle (bool): Whether to shuffle the dataset before splitting\n","\n","    Returns:\n","        Traning and validation datasets as two separated dicts\n","    \"\"\"\n","\n","    # 1. Get the list of all image names\n","    all_images = list(caption_data.keys())\n","    all_images = np.array(all_images)\n","    train_idx, val_idx, test_idx = [], [], []\n","\n","    for image in info_data['images']:\n","      if image['split'] == 'train':\n","        train_idx.append(np.where(all_images==IMAGES_PATH + '/' + image['filename']))\n","      if image['split'] == 'val':\n","        val_idx.append(np.where(all_images==IMAGES_PATH + '/' + image['filename']))\n","      if image['split'] == 'test':\n","        test_idx.append(np.where(all_images==IMAGES_PATH + '/' + image['filename']))\n","\n","\n","    training_data = {\n","        img_name[0][0]: caption_data[img_name[0][0]] for img_name in all_images[train_idx]\n","    }\n","    validation_data = {\n","        img_name[0][0]: caption_data[img_name[0][0]] for img_name in all_images[val_idx]\n","    }\n","    test_data = {\n","        img_name[0][0]: caption_data[img_name[0][0]] for img_name in all_images[test_idx]\n","    }\n","\n","    # 4. Return the splits\n","    return training_data, validation_data, test_data\n","\n","\n","# Load the dataset\n","captions_mapping, text_data = load_captions_data(captions)\n","\n","# Split the dataset into training and validation sets\n","train_data, valid_data, test_data = train_val_split_karpathy(captions_mapping, info)\n","print(\"Number of training samples: \", len(train_data))\n","print(\"Number of validation samples: \", len(valid_data))\n","print(\"Number of test samples: \", len(test_data))"]},{"cell_type":"code","source":["train_data_text = []\n","for i in train_data.values(): train_data_text.extend(i)"],"metadata":{"id":"sn-KGN_dPUrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_data_text), len(text_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPGQ354UPq-J","executionInfo":{"status":"ok","timestamp":1679759946674,"user_tz":180,"elapsed":18,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"}},"outputId":"2acc07a4-bc7f-4cd8-9178-e6b72709c4df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(145000, 155070)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"yTSnr7Mp_qsh"},"source":["## Vectorizing the text data\n","\n","We'll use the `TextVectorization` layer to vectorize the text data,\n","that is to say, to turn the\n","original strings into integer sequences where each integer represents the index of\n","a word in a vocabulary. We will use a custom string standardization scheme\n","(strip punctuation characters except `<` and `>`) and the default\n","splitting scheme (split on whitespace)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3756,"status":"ok","timestamp":1680705171015,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"TVkODZ4z5EzD","outputId":"fc96f4c5-3aa1-4719-b494-37ffd27c66f4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7400, 10586)"]},"metadata":{},"execution_count":9}],"source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~0123456789')\n","tokenizer.fit_on_texts(train_data_text)\n","tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'\n","VOCAB_SIZE=0\n","out=0\n","for word, q in ast.literal_eval(tokenizer.get_config()['word_counts']).items():\n","  if q > 4: VOCAB_SIZE-=-1\n","  else: out-=-1\n","VOCAB_SIZE, out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yt521A5VLQqF"},"outputs":[],"source":["d=ast.literal_eval(tokenizer.get_config()['word_counts'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejyqSkHy_qsh"},"outputs":[],"source":["\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n","strip_chars = strip_chars.replace(\"<\", \"\")\n","strip_chars = strip_chars.replace(\">\", \"\")\n","\n","vectorization = TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode=\"int\",\n","    output_sequence_length=SEQ_LENGTH,\n","    standardize=custom_standardization,\n","    pad_to_max_tokens=True\n",")\n","vectorization.adapt(train_data_text)\n","\n","# Data augmentation for image data\n","image_augmentation = keras.Sequential(\n","    [\n","                layers.RandomTranslation(height_factor=(0.1), width_factor=(0.1),\n","                                 fill_mode=\"nearest\",),\n","        layers.RandomZoom(height_factor=(0.1), fill_mode=\"nearest\"),\n","        layers.RandomRotation(0.1, fill_mode=\"nearest\"),\n","        layers.RandomHeight(0.2),\n","        layers.RandomWidth(0.2),\n","        layers.RandomContrast(0.1),\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"v4YJJ2-B_qsh"},"source":["## Building a `tf.data.Dataset` pipeline for training\n","\n","We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n","The pipeline consists of two steps:\n","\n","1. Read the image from the disk\n","2. Tokenize all the five captions corresponding to the image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZqZE-8k_qsi"},"outputs":[],"source":["\n","def decode_and_resize(img_path):\n","    img = tf.io.read_file(img_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, IMAGE_SIZE)\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","    return img\n","\n","\n","def process_input(img_path, captions):\n","    return decode_and_resize(img_path), vectorization(captions)\n","\n","\n","def make_dataset(images, captions):\n","    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n","    dataset = dataset.shuffle(len(images))\n","    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n","\n","    return dataset\n","\n","\n","# Pass the list of images and the list of corresponding captions\n","train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n","\n","valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqjdE6MNWPyw"},"outputs":[],"source":["def positional_encoding(length, depth):\n","  depth = depth/2\n","\n","  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","  angle_rates = 1 / (10000**depths)         # (1, depth)\n","  angle_rads = positions * angle_rates      # (pos, depth)\n","\n","  pos_encoding = np.concatenate(\n","      [np.sin(angle_rads), np.cos(angle_rads)],\n","      axis=-1)\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvENyMNu_qsi"},"outputs":[],"source":["\n","def get_cnn_model():\n","    base_model = EfficientNetV2L(\n","        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n","    )\n","    # We freeze our feature extractor\n","    base_model.trainable = False\n","    base_model_out = base_model.output\n","    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n","    cnn_model = keras.models.Model(base_model.input, base_model_out)\n","    return cnn_model\n","\n","\n","class TransformerEncoderBlock(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, rate=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.dense_1 = layers.Dense(dense_dim, activation=\"relu\")\n","        self.dense_2 = layers.Dense(embed_dim)\n","        self.dropout = layers.Dropout(rate)\n","\n","    def call(self, inputs, training, mask=None):\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=None,\n","            training=training,\n","        )\n","\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","        out_2 = self.dense_1(out_1)\n","        out_3 = self.dense_2(out_2) #512\n","        out_3 = self.dropout(out_3, training=training)\n","\n","        return self.layernorm_2(out_1 + out_3)\n","\n","class PositionalEmbedding_IMG(layers.Layer):\n","    def __init__(self, sequence_length, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.pos_encoding = positional_encoding(length=sequence_length, depth=embed_dim)\n","        self.sequence_length = sequence_length\n","        self.embed_dim = embed_dim\n","\n","    def call(self, x):\n","        length = tf.shape(x)[1]\n","        x = x + self.pos_encoding[tf.newaxis, :length, :]\n","        return x\n","\n","class Encoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, num_layers, rate=0.4, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_layers = num_layers\n","        self.embedding = layers.Dense(embed_dim, activation='relu')\n","        self.pos_embedding = PositionalEmbedding_IMG(\n","            embed_dim=embed_dim, sequence_length=r_size * c_size\n","        )\n","        self.enc_layers = [TransformerEncoderBlock(embed_dim, dense_dim, num_heads)\n","                       for _ in range(num_layers)]\n","        self.dropout = layers.Dropout(rate)\n","    def call(self, x, training, mask=None):\n","        x = self.embedding(x)\n","        x = self.pos_embedding(x)\n","        x = self.dropout(x, training=training)\n","        for i in range(self.num_layers):\n","          x = self.enc_layers[i](x, training, mask)\n","\n","        return x\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.pos_encoding = positional_encoding(length=sequence_length, depth=embed_dim)\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, x):\n","        length = tf.shape(x)[1]\n","        x = self.token_embeddings(x)\n","        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n","        x = x + self.pos_encoding[tf.newaxis, :length, :]\n","        return x\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","class TransformerDecoderBlock(layers.Layer):\n","    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.ff_dim = ff_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n","        )\n","        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n","        self.ffn_layer_2 = layers.Dense(embed_dim)\n","\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","\n","        self.dropout_1 = layers.Dropout(0.8)\n","        self.dropout_2 = layers.Dropout(0.8)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, training, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n","            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n","            combined_mask = tf.minimum(combined_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=combined_mask,\n","            training=training,\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2, attention_scores = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","            training=training,\n","            return_attention_scores=True\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        ffn_out = self.ffn_layer_1(out_2)\n","        ffn_out = self.ffn_layer_2(ffn_out)\n","        ffn_out = self.dropout_2(ffn_out, training=training)\n","        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n","\n","        return ffn_out, attention_scores\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n","\n","class Decoder(layers.Layer):\n","  def __init__(self, embed_dim, dense_dim, num_heads, n_layers, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.n_layers = n_layers\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n","        self.embedding = PositionalEmbedding(\n","            embed_dim=embed_dim, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n","        )\n","        self.decoder_layers = [TransformerDecoderBlock(embed_dim=embed_dim, ff_dim=FF_DIM, num_heads=heads)\n","                                for _ in range(n_layers)]\n","  def call(self, inputs, encoder_outputs, training, mask=None):\n","        inputs = self.embedding(inputs)\n","\n","        for i in range(self.n_layers):\n","            inputs, weights = self.decoder_layers[i](inputs, encoder_outputs, training, mask)\n","        pred = self.out(inputs)\n","        return pred\n","  def pred(self, inputs, encoder_outputs, training, mask=None):\n","        inputs = self.embedding(inputs)\n","\n","        layers_att_weights = []\n","        for i in range(self.n_layers):\n","            inputs, weights = self.decoder_layers[i](inputs, encoder_outputs, training, mask)\n","            layers_att_weights.append(weights)\n","        pred = self.out(inputs)\n","        return pred, layers_att_weights\n","\n","class ImageCaptioningModel(keras.Model):\n","    def __init__(\n","        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None,\n","    ):\n","        super().__init__()\n","        self.cnn_model = cnn_model\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n","        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n","        self.num_captions_per_image = num_captions_per_image\n","        self.image_aug = image_aug\n","\n","    def calculate_loss(self, y_true, y_pred, mask):\n","        loss = self.loss(y_true, y_pred)\n","        mask = tf.cast(mask, dtype=loss.dtype)\n","        loss *= mask\n","        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n","\n","    def calculate_accuracy(self, y_true, y_pred, mask):\n","        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n","        accuracy = tf.math.logical_and(mask, accuracy)\n","        accuracy = tf.cast(accuracy, dtype=tf.float32)\n","        mask = tf.cast(mask, dtype=tf.float32)\n","        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n","\n","    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n","        encoder_out = self.encoder(img_embed, training=training)\n","        batch_seq_inp = batch_seq[:, :-1]\n","        batch_seq_true = batch_seq[:, 1:]\n","        mask = tf.math.not_equal(batch_seq_true, 0)\n","        batch_seq_pred = self.decoder(\n","            batch_seq_inp, encoder_out, training=training, mask=mask\n","        )\n","        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n","        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n","        return loss, acc\n","\n","    def train_step(self, batch_data):\n","        batch_img, batch_seq = batch_data\n","        batch_loss = 0\n","        batch_acc = 0\n","\n","        if self.image_aug:\n","            batch_img = self.image_aug(batch_img)\n","\n","        # 1. Get image embeddings\n","        img_embed = self.cnn_model(batch_img)\n","\n","        # 2. Pass each of the five captions one by one to the decoder\n","        # along with the encoder outputs and compute the loss as well as accuracy\n","        # for each caption.\n","        for i in range(self.num_captions_per_image):\n","            with tf.GradientTape() as tape:\n","                loss, acc = self._compute_caption_loss_and_acc(\n","                    img_embed, batch_seq[:, i, :], training=True\n","                )\n","\n","                # 3. Update loss and accuracy\n","                batch_loss += loss\n","                batch_acc += acc\n","\n","            # 4. Get the list of all the trainable weights\n","            train_vars = (\n","                self.encoder.trainable_variables + self.decoder.trainable_variables\n","            )\n","\n","            # 5. Get the gradients\n","            grads = tape.gradient(loss, train_vars)\n","\n","            # 6. Update the trainable weights\n","            self.optimizer.apply_gradients(zip(grads, train_vars))\n","\n","        # 7. Update the trackers\n","        batch_acc /= float(self.num_captions_per_image)\n","        self.loss_tracker.update_state(batch_loss)\n","        self.acc_tracker.update_state(batch_acc)\n","\n","        # 8. Return the loss and accuracy values\n","        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n","\n","    def test_step(self, batch_data):\n","        batch_img, batch_seq = batch_data\n","        batch_loss = 0\n","        batch_acc = 0\n","\n","        # 1. Get image embeddings\n","        img_embed = self.cnn_model(batch_img)\n","\n","        # 2. Pass each of the five captions one by one to the decoder\n","        # along with the encoder outputs and compute the loss as well as accuracy\n","        # for each caption.\n","        for i in range(self.num_captions_per_image):\n","            loss, acc = self._compute_caption_loss_and_acc(\n","                img_embed, batch_seq[:, i, :], training=False\n","            )\n","\n","            # 3. Update batch loss and batch accuracy\n","            batch_loss += loss\n","            batch_acc += acc\n","\n","        batch_acc /= float(self.num_captions_per_image)\n","\n","        # 4. Update the trackers\n","        self.loss_tracker.update_state(batch_loss)\n","        self.acc_tracker.update_state(batch_acc)\n","\n","        # 5. Return the loss and accuracy values\n","        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n","\n","    @property\n","    def metrics(self):\n","        # We need to list our metrics here so the `reset_states()` can be\n","        # called automatically.\n","        return [self.loss_tracker, self.acc_tracker]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11677,"status":"ok","timestamp":1680705209170,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"YCdgZU-Qn_8H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a724b779-f196-4d79-b4cb-e9f1b9245749"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n","473178112/473176280 [==============================] - 3s 0us/step\n","473186304/473176280 [==============================] - 3s 0us/step\n"]}],"source":["# Dimension for the image embeddings and token embeddings\n","EMBED_DIM = 512\n","r_size, c_size=12, 12\n","# Per-layer units in the feed-forward network\n","FF_DIM = 2048\n","\n","heads = 6\n","cnn_model = get_cnn_model()\n","encoder = Encoder(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=heads, num_layers=2)\n","decoder = Decoder(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=heads, n_layers=2)\n","caption_model = ImageCaptioningModel(\n","    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",")\n","last_epoch=0"]},{"cell_type":"markdown","metadata":{"id":"Q8NZMjP6_qsj"},"source":["## Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzasuDuu_qsk"},"outputs":[],"source":["\n","# Define the loss function\n","cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=False, reduction=\"none\"\n",")\n","\n","# EarlyStopping criteria\n","early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1)\n","checkpoint_filepath = '/content/drive/MyDrive/Mestrado/ImageCaptioning/pesos/model_V2EN.{epoch:02d}-{val_loss:.4f}.tf'\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_best_only=True)\n","\n","# Learning Rate Scheduler for the optimizer\n","class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, post_warmup_learning_rate, warmup_steps):\n","        super().__init__()\n","        self.post_warmup_learning_rate = post_warmup_learning_rate\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        global_step = tf.cast(step, tf.float32)\n","        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n","        warmup_progress = global_step / warmup_steps\n","        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n","        return tf.cond(\n","            global_step < warmup_steps,\n","            lambda: warmup_learning_rate,\n","            lambda: self.post_warmup_learning_rate,\n","        )\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=10000):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, dtype=tf.float32)\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","EPOCHS=90\n","# Create a learning rate schedule\n","learning_rate = LRSchedule(post_warmup_learning_rate=1e-5, warmup_steps=20000)\n","\n","# Compile the model\n","caption_model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss=cross_entropy)"]},{"cell_type":"code","source":["# Fit the model\n","H=caption_model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=valid_dataset,\n","    callbacks=[early_stopping, model_checkpoint_callback],\n","    initial_epoch=last_epoch\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJWEHlvKjLBf","outputId":"7fe0a483-5b4d-471f-99bb-19150f20400c","executionInfo":{"status":"ok","timestamp":1680755372179,"user_tz":180,"elapsed":14126174,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 34/90\n","454/454 [==============================] - 2337s 5s/step - loss: 12.6797 - acc: 0.4488 - val_loss: 14.0248 - val_acc: 0.4115\n","Epoch 35/90\n","454/454 [==============================] - 2270s 5s/step - loss: 12.5874 - acc: 0.4516 - val_loss: 13.9598 - val_acc: 0.4144\n","Epoch 36/90\n","454/454 [==============================] - 2269s 5s/step - loss: 12.4902 - acc: 0.4540 - val_loss: 13.9254 - val_acc: 0.4143\n","Epoch 37/90\n","454/454 [==============================] - 2264s 5s/step - loss: 12.3989 - acc: 0.4564 - val_loss: 13.9775 - val_acc: 0.4143\n","Epoch 38/90\n","454/454 [==============================] - 2238s 5s/step - loss: 12.3065 - acc: 0.4578 - val_loss: 13.8777 - val_acc: 0.4177\n","Epoch 39/90\n","454/454 [==============================] - 2231s 5s/step - loss: 12.2238 - acc: 0.4598 - val_loss: 13.9661 - val_acc: 0.4138\n","Epoch 40/90\n","454/454 [==============================] - 2229s 5s/step - loss: 12.1357 - acc: 0.4628 - val_loss: 13.8927 - val_acc: 0.4126\n","Epoch 41/90\n","454/454 [==============================] - 2237s 5s/step - loss: 12.0526 - acc: 0.4638 - val_loss: 13.8421 - val_acc: 0.4135\n","Epoch 42/90\n","454/454 [==============================] - 2262s 5s/step - loss: 11.9618 - acc: 0.4673 - val_loss: 13.8547 - val_acc: 0.4154\n","Epoch 43/90\n","454/454 [==============================] - 2232s 5s/step - loss: 11.8856 - acc: 0.4682 - val_loss: 13.9126 - val_acc: 0.4107\n","Epoch 44/90\n","454/454 [==============================] - 2270s 5s/step - loss: 11.8030 - acc: 0.4721 - val_loss: 13.8420 - val_acc: 0.4129\n","Epoch 45/90\n","454/454 [==============================] - 2270s 5s/step - loss: 11.7214 - acc: 0.4741 - val_loss: 13.8372 - val_acc: 0.4139\n","Epoch 46/90\n","454/454 [==============================] - 2231s 5s/step - loss: 11.6412 - acc: 0.4746 - val_loss: 13.8529 - val_acc: 0.4126\n","Epoch 47/90\n","454/454 [==============================] - 2236s 5s/step - loss: 11.5673 - acc: 0.4775 - val_loss: 13.8327 - val_acc: 0.4178\n","Epoch 48/90\n","454/454 [==============================] - 2269s 5s/step - loss: 11.4917 - acc: 0.4796 - val_loss: 13.8297 - val_acc: 0.4154\n","Epoch 49/90\n","454/454 [==============================] - 2263s 5s/step - loss: 11.4124 - acc: 0.4821 - val_loss: 13.9037 - val_acc: 0.4106\n","Epoch 50/90\n","454/454 [==============================] - 2270s 5s/step - loss: 11.3408 - acc: 0.4834 - val_loss: 13.7564 - val_acc: 0.4153\n","Epoch 51/90\n","454/454 [==============================] - 2264s 5s/step - loss: 11.2596 - acc: 0.4862 - val_loss: 13.8704 - val_acc: 0.4141\n","Epoch 52/90\n","454/454 [==============================] - 2263s 5s/step - loss: 11.1957 - acc: 0.4882 - val_loss: 13.8562 - val_acc: 0.4126\n","Epoch 53/90\n","454/454 [==============================] - 2265s 5s/step - loss: 11.1216 - acc: 0.4900 - val_loss: 13.8774 - val_acc: 0.4127\n","Epoch 54/90\n","454/454 [==============================] - 2264s 5s/step - loss: 11.0462 - acc: 0.4921 - val_loss: 13.8691 - val_acc: 0.4127\n","Epoch 55/90\n","454/454 [==============================] - 2265s 5s/step - loss: 10.9772 - acc: 0.4942 - val_loss: 13.9039 - val_acc: 0.4092\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/image_captioning.ipynb","timestamp":1663259576283}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}