{"cells":[{"cell_type":"markdown","source":["**Acknowledgments**: This code was mainly based on the tutorial by Aakash Kumar Nain, available [here](https://keras.io/examples/vision/image_captioning/), to whom we extend our thanks for their contribution and for making the material publicly available."],"metadata":{"id":"IS_yfdpnBf5G"}},{"cell_type":"markdown","metadata":{"id":"42x0bbTh_qsc"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTiNJAZqHt84"},"outputs":[],"source":["pip install tensorflow==2.8.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEfKshQD_qsd"},"outputs":[],"source":["import os, json\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import ast\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications import EfficientNetV2L\n","from tensorflow.keras.layers import TextVectorization\n","\n","\n","seed = 111\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30333,"status":"ok","timestamp":1682372008353,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"ZkcCSjXWAH1B","outputId":"73cc933e-3c50-4cb6-9ccc-4c263810e077"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BipKr2a4_qse"},"outputs":[],"source":["with open('/content/drive/MyDrive/Mestrado/ImageCaptioning/datasets/flickr30k/dataset_flickr30k_all.json', 'r') as captionsJson:\n","  captions = json.load(captionsJson)\n","with open('/content/drive/MyDrive/Mestrado/ImageCaptioning/datasets/flickr30k/dataset_flickr30k.json', 'r') as captionsJson:\n","  info = json.load(captionsJson)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lh_oJH-rAYJe"},"outputs":[],"source":["!tar -xf /content/drive/MyDrive/Mestrado/ImageCaptioning/datasets/flickr30k/flickr30k-images.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPrBgwDT_qse"},"outputs":[],"source":["\n","# Path to the images\n","IMAGES_PATH = \"flickr30k-images\"\n","\n","# Desired image dimensions\n","IMAGE_SIZE = (384, 384)\n","\n","# Fixed length allowed for any sequence\n","SEQ_LENGTH = 25\n","\n","# Other training parameters\n","BATCH_SIZE = 64\n","AUTOTUNE = tf.data.AUTOTUNE"]},{"cell_type":"markdown","metadata":{"id":"1enfCX9q_qsf"},"source":["## Preparing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18563,"status":"ok","timestamp":1682372071735,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"paYGaujC_qsg","outputId":"2f5b8b3e-8169-45e8-f8b4-93dabd2ef281"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training samples:  29000\n","Number of validation samples:  1014\n","Number of test samples:  1000\n"]}],"source":["\n","def load_captions_data(captionsfile):\n","    \"\"\"Loads captions (text) data and maps them to corresponding images.\n","\n","    Args:\n","        filename: Path to the text file containing caption data.\n","\n","    Returns:\n","        caption_mapping: Dictionary mapping image names and the corresponding captions\n","        text_data: List containing all the available captions\n","    \"\"\"\n","    caption_mapping = {}\n","    text_data = []\n","    images_to_skip = set()\n","\n","    for img_name, captions in captionsfile.items():\n","        img_name = os.path.join(IMAGES_PATH, img_name)\n","        for caption in captions:\n","          tokens = caption.strip().split()\n","\n","          if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n","              # We will add a start and an end token to each caption\n","              text_data.append(caption)\n","\n","              if img_name in caption_mapping:\n","                  caption_mapping[img_name].append(caption)\n","              else:\n","                  caption_mapping[img_name] = [caption]\n","\n","    return caption_mapping, text_data\n","\n","def train_val_split_karpathy(caption_data, info_data):\n","    \"\"\"Split the captioning dataset into train and validation sets.\n","\n","    Args:\n","        caption_data (dict): Dictionary containing the mapped caption data\n","        train_size (float): Fraction of all the full dataset to use as training data\n","        shuffle (bool): Whether to shuffle the dataset before splitting\n","\n","    Returns:\n","        Traning and validation datasets as two separated dicts\n","    \"\"\"\n","\n","    # 1. Get the list of all image names\n","    all_images = list(caption_data.keys())\n","    all_images = np.array(all_images)\n","    train_idx, val_idx, test_idx = [], [], []\n","\n","    for image in info_data['images']:\n","      if image['split'] == 'train':\n","        train_idx.append(np.where(all_images==IMAGES_PATH + '/' + image['filename']))\n","      if image['split'] == 'val':\n","        val_idx.append(np.where(all_images==IMAGES_PATH + '/' + image['filename']))\n","      if image['split'] == 'test':\n","        test_idx.append(np.where(all_images==IMAGES_PATH + '/' + image['filename']))\n","\n","\n","    training_data = {\n","        img_name[0][0]: caption_data[img_name[0][0]] for img_name in all_images[train_idx]\n","    }\n","    validation_data = {\n","        img_name[0][0]: caption_data[img_name[0][0]] for img_name in all_images[val_idx]\n","    }\n","    test_data = {\n","        img_name[0][0]: caption_data[img_name[0][0]] for img_name in all_images[test_idx]\n","    }\n","\n","    # 4. Return the splits\n","    return training_data, validation_data, test_data\n","\n","\n","# Load the dataset\n","captions_mapping, text_data = load_captions_data(captions)\n","\n","# Split the dataset into training and validation sets\n","train_data, valid_data, test_data = train_val_split_karpathy(captions_mapping, info)\n","print(\"Number of training samples: \", len(train_data))\n","print(\"Number of validation samples: \", len(valid_data))\n","print(\"Number of test samples: \", len(test_data))"]},{"cell_type":"code","source":["train_data_text = []\n","for i in train_data.values(): train_data_text.extend(i)"],"metadata":{"id":"sn-KGN_dPUrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_data_text), len(text_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPGQ354UPq-J","executionInfo":{"status":"ok","timestamp":1679759946674,"user_tz":180,"elapsed":18,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"}},"outputId":"2acc07a4-bc7f-4cd8-9178-e6b72709c4df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(145000, 155070)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"yTSnr7Mp_qsh"},"source":["## Vectorizing the text data\n","\n","We'll use the `TextVectorization` layer to vectorize the text data,\n","that is to say, to turn the\n","original strings into integer sequences where each integer represents the index of\n","a word in a vocabulary. We will use a custom string standardization scheme\n","(strip punctuation characters except `<` and `>`) and the default\n","splitting scheme (split on whitespace)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2935,"status":"ok","timestamp":1680276571599,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"TVkODZ4z5EzD","outputId":"b6a7ca6c-732c-4b56-f94f-5c6c8521fbf5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8056, 13313)"]},"metadata":{},"execution_count":9}],"source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~0123456789')\n","tokenizer.fit_on_texts(train_data_text)\n","tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'\n","VOCAB_SIZE=0\n","out=0\n","for word, q in ast.literal_eval(tokenizer.get_config()['word_counts']).items():\n","  if q > 4: VOCAB_SIZE-=-1\n","  else: out-=-1\n","VOCAB_SIZE, out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yt521A5VLQqF"},"outputs":[],"source":["d=ast.literal_eval(tokenizer.get_config()['word_counts'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejyqSkHy_qsh"},"outputs":[],"source":["\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n","strip_chars = strip_chars.replace(\"<\", \"\")\n","strip_chars = strip_chars.replace(\">\", \"\")\n","\n","vectorization = TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode=\"int\",\n","    output_sequence_length=SEQ_LENGTH,\n","    standardize=custom_standardization,\n","    pad_to_max_tokens=True\n",")\n","vectorization.adapt(train_data_text)\n","\n","# Data augmentation for image data\n","image_augmentation = keras.Sequential(\n","    [\n","\n","        layers.RandomTranslation(height_factor=(0.1), width_factor=(0.1),\n","                                 fill_mode=\"nearest\",),\n","        layers.RandomZoom(height_factor=(0.1), fill_mode=\"nearest\"),\n","        layers.RandomRotation(0.1, fill_mode=\"nearest\"),\n","        layers.RandomHeight(0.2),\n","        layers.RandomWidth(0.2),\n","        layers.RandomContrast(0.1),\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"v4YJJ2-B_qsh"},"source":["## Building a `tf.data.Dataset` pipeline for training\n","\n","We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n","The pipeline consists of two steps:\n","\n","1. Read the image from the disk\n","2. Tokenize all the five captions corresponding to the image"]},{"cell_type":"code","source":["images, captions = list(train_data.keys()), list(train_data.values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PTOtKdvlNLiF","executionInfo":{"status":"ok","timestamp":1682372362217,"user_tz":180,"elapsed":14,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"}},"outputId":"7632ae95-07ba-416c-eb74-cd79a165ead8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.string, name=None))>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZqZE-8k_qsi"},"outputs":[],"source":["\n","def decode_and_resize(img_path):\n","    img = tf.io.read_file(img_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, IMAGE_SIZE)\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","    return img\n","\n","\n","def process_input(img_path, captions):\n","    return decode_and_resize(img_path), vectorization(captions)\n","\n","\n","def make_dataset(images, captions):\n","    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n","    dataset = dataset.shuffle(len(images))\n","    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n","\n","    return dataset\n","\n","\n","# Pass the list of images and the list of corresponding captions\n","train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n","\n","valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqjdE6MNWPyw"},"outputs":[],"source":["def positional_encoding(length, depth):\n","  depth = depth/2\n","\n","  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","  angle_rates = 1 / (10000**depths)         # (1, depth)\n","  angle_rads = positions * angle_rates      # (pos, depth)\n","\n","  pos_encoding = np.concatenate(\n","      [np.sin(angle_rads), np.cos(angle_rads)],\n","      axis=-1)\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvENyMNu_qsi"},"outputs":[],"source":["\n","def get_cnn_model():\n","    base_model = EfficientNetV2L(\n","        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n","    )\n","    # We freeze our feature extractor\n","    base_model.trainable = False\n","    base_model_out = base_model.output\n","    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n","    cnn_model = keras.models.Model(base_model.input, base_model_out)\n","    return cnn_model\n","\n","\n","class TransformerEncoderBlock(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, rate=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.dense_1 = layers.Dense(dense_dim, activation=\"relu\")\n","        self.dense_2 = layers.Dense(embed_dim)\n","        self.dropout = layers.Dropout(rate)\n","\n","    def call(self, inputs, training, mask=None):\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=None,\n","            training=training,\n","        )\n","\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","        out_2 = self.dense_1(out_1)\n","        out_3 = self.dense_2(out_2)\n","        out_3 = self.dropout(out_3, training=training)\n","\n","        return self.layernorm_2(out_1 + out_3)\n","\n","class PositionalEmbedding_IMG(layers.Layer):\n","    def __init__(self, sequence_length, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.pos_encoding = positional_encoding(length=sequence_length, depth=embed_dim)\n","        self.sequence_length = sequence_length\n","        self.embed_dim = embed_dim\n","\n","    def call(self, x):\n","        length = tf.shape(x)[1]\n","        x = x + self.pos_encoding[tf.newaxis, :length, :]\n","        return x\n","\n","class Encoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, num_layers, rate=0.4, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_layers = num_layers\n","        self.embedding = layers.Dense(embed_dim, activation='relu')\n","        self.pos_embedding = PositionalEmbedding_IMG(\n","            embed_dim=embed_dim, sequence_length=r_size * c_size\n","        )\n","        self.enc_layers = [TransformerEncoderBlock(embed_dim, dense_dim, num_heads)\n","                       for _ in range(num_layers)]\n","        self.dropout = layers.Dropout(rate)\n","    def call(self, x, training, mask=None):\n","        x = self.embedding(x)\n","        x = self.pos_embedding(x)\n","        x = self.dropout(x, training=training)\n","        for i in range(self.num_layers):\n","          x = self.enc_layers[i](x, training, mask)\n","\n","        return x\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.pos_encoding = positional_encoding(length=sequence_length, depth=embed_dim)\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, x):\n","        length = tf.shape(x)[1]\n","        x = self.token_embeddings(x)\n","        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n","        x = x + self.pos_encoding[tf.newaxis, :length, :]\n","        return x\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","class TransformerDecoderBlock(layers.Layer):\n","    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.ff_dim = ff_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n","        )\n","        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n","        self.ffn_layer_2 = layers.Dense(embed_dim)\n","\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","\n","        self.dropout_1 = layers.Dropout(0.8)\n","        self.dropout_2 = layers.Dropout(0.8)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, training, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n","            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n","            combined_mask = tf.minimum(combined_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=combined_mask,\n","            training=training,\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2, attention_scores = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","            training=training,\n","            return_attention_scores=True\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        ffn_out = self.ffn_layer_1(out_2)\n","        ffn_out = self.ffn_layer_2(ffn_out)\n","        ffn_out = self.dropout_2(ffn_out, training=training)\n","        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n","\n","        return ffn_out, attention_scores\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n","\n","class Decoder(layers.Layer):\n","  def __init__(self, embed_dim, dense_dim, num_heads, n_layers, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.n_layers = n_layers\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n","        self.embedding = PositionalEmbedding(\n","            embed_dim=embed_dim, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n","        )\n","        self.decoder_layers = [TransformerDecoderBlock(embed_dim=embed_dim, ff_dim=FF_DIM, num_heads=heads)\n","                                for _ in range(n_layers)]\n","  def call(self, inputs, encoder_outputs, training, mask=None):\n","        inputs = self.embedding(inputs)\n","\n","        for i in range(self.n_layers):\n","            inputs, weights = self.decoder_layers[i](inputs, encoder_outputs, training, mask)\n","        pred = self.out(inputs)\n","        return pred\n","  def pred(self, inputs, encoder_outputs, training, mask=None):\n","        inputs = self.embedding(inputs)\n","\n","        layers_att_weights = []\n","        for i in range(self.n_layers):\n","            inputs, weights = self.decoder_layers[i](inputs, encoder_outputs, training, mask)\n","            layers_att_weights.append(weights)\n","        pred = self.out(inputs)\n","        return pred, layers_att_weights\n","\n","class ImageCaptioningModel(keras.Model):\n","    def __init__(\n","        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None,\n","    ):\n","        super().__init__()\n","        self.cnn_model = cnn_model\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n","        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n","        self.num_captions_per_image = num_captions_per_image\n","        self.image_aug = image_aug\n","\n","    def calculate_loss(self, y_true, y_pred, mask):\n","        loss = self.loss(y_true, y_pred)\n","        mask = tf.cast(mask, dtype=loss.dtype)\n","        loss *= mask\n","        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n","\n","    def calculate_accuracy(self, y_true, y_pred, mask):\n","        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n","        accuracy = tf.math.logical_and(mask, accuracy)\n","        accuracy = tf.cast(accuracy, dtype=tf.float32)\n","        mask = tf.cast(mask, dtype=tf.float32)\n","        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n","\n","    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n","        encoder_out = self.encoder(img_embed, training=training)\n","        batch_seq_inp = batch_seq[:, :-1]\n","        batch_seq_true = batch_seq[:, 1:]\n","        mask = tf.math.not_equal(batch_seq_true, 0)\n","        batch_seq_pred = self.decoder(\n","            batch_seq_inp, encoder_out, training=training, mask=mask\n","        )\n","        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n","        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n","        return loss, acc\n","\n","    def train_step(self, batch_data):\n","        batch_img, batch_seq = batch_data\n","        batch_loss = 0\n","        batch_acc = 0\n","\n","        if self.image_aug:\n","            batch_img = self.image_aug(batch_img)\n","\n","        # 1. Get image embeddings\n","        img_embed = self.cnn_model(batch_img)\n","\n","        # 2. Pass each of the five captions one by one to the decoder\n","        # along with the encoder outputs and compute the loss as well as accuracy\n","        # for each caption.\n","        for i in range(self.num_captions_per_image):\n","            with tf.GradientTape() as tape:\n","                loss, acc = self._compute_caption_loss_and_acc(\n","                    img_embed, batch_seq[:, i, :], training=True\n","                )\n","\n","                # 3. Update loss and accuracy\n","                batch_loss += loss\n","                batch_acc += acc\n","\n","            # 4. Get the list of all the trainable weights\n","            train_vars = (\n","                self.encoder.trainable_variables + self.decoder.trainable_variables\n","            )\n","\n","            # 5. Get the gradients\n","            grads = tape.gradient(loss, train_vars)\n","\n","            # 6. Update the trainable weights\n","            self.optimizer.apply_gradients(zip(grads, train_vars))\n","\n","        # 7. Update the trackers\n","        batch_acc /= float(self.num_captions_per_image)\n","        self.loss_tracker.update_state(batch_loss)\n","        self.acc_tracker.update_state(batch_acc)\n","\n","        # 8. Return the loss and accuracy values\n","        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n","\n","    def test_step(self, batch_data):\n","        batch_img, batch_seq = batch_data\n","        batch_loss = 0\n","        batch_acc = 0\n","\n","        # 1. Get image embeddings\n","        img_embed = self.cnn_model(batch_img)\n","\n","        # 2. Pass each of the five captions one by one to the decoder\n","        # along with the encoder outputs and compute the loss as well as accuracy\n","        # for each caption.\n","        for i in range(self.num_captions_per_image):\n","            loss, acc = self._compute_caption_loss_and_acc(\n","                img_embed, batch_seq[:, i, :], training=False\n","            )\n","\n","            # 3. Update batch loss and batch accuracy\n","            batch_loss += loss\n","            batch_acc += acc\n","\n","        batch_acc /= float(self.num_captions_per_image)\n","\n","        # 4. Update the trackers\n","        self.loss_tracker.update_state(batch_loss)\n","        self.acc_tracker.update_state(batch_acc)\n","\n","        # 5. Return the loss and accuracy values\n","        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n","\n","    @property\n","    def metrics(self):\n","        # We need to list our metrics here so the `reset_states()` can be\n","        # called automatically.\n","        return [self.loss_tracker, self.acc_tracker]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":28119,"status":"ok","timestamp":1680276607838,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"},"user_tz":180},"id":"YCdgZU-Qn_8H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de5914d8-cf38-43cc-da00-40b2804e7ad8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n","473178112/473176280 [==============================] - 20s 0us/step\n","473186304/473176280 [==============================] - 20s 0us/step\n"]}],"source":["# Dimension for the image embeddings and token embeddings\n","EMBED_DIM = 512\n","r_size, c_size=12, 12\n","# Per-layer units in the feed-forward network\n","FF_DIM = 2048\n","\n","heads = 6\n","cnn_model = get_cnn_model()\n","encoder = Encoder(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=heads, num_layers=2)\n","decoder = Decoder(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=heads, n_layers=2)\n","caption_model = ImageCaptioningModel(\n","    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",")\n","last_epoch=0"]},{"cell_type":"markdown","metadata":{"id":"Q8NZMjP6_qsj"},"source":["## Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzasuDuu_qsk"},"outputs":[],"source":["\n","# Define the loss function\n","cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=False, reduction=\"none\"\n",")\n","\n","# EarlyStopping criteria\n","early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1)\n","checkpoint_filepath = '/content/drive/MyDrive/Mestrado/ImageCaptioning/pesos/model_V2.{epoch:02d}-{val_loss:.4f}.tf'\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_best_only=True)\n","\n","# Learning Rate Scheduler for the optimizer\n","class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, post_warmup_learning_rate, warmup_steps):\n","        super().__init__()\n","        self.post_warmup_learning_rate = post_warmup_learning_rate\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        global_step = tf.cast(step, tf.float32)\n","        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n","        warmup_progress = global_step / warmup_steps\n","        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n","        return tf.cond(\n","            global_step < warmup_steps,\n","            lambda: warmup_learning_rate,\n","            lambda: self.post_warmup_learning_rate,\n","        )\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=10000):\n","    super().__init__()\n","\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, dtype=tf.float32)\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","EPOCHS=90\n","learning_rate = LRSchedule(post_warmup_learning_rate=1e-5, warmup_steps=20000)\n","\n","# Compile the model\n","caption_model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss=cross_entropy)"]},{"cell_type":"code","source":["# Fit the model\n","H=caption_model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=valid_dataset,\n","    callbacks=[early_stopping, model_checkpoint_callback],\n","    initial_epoch=last_epoch\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJWEHlvKjLBf","outputId":"59a52f45-9b81-49f9-8fc6-0afcfb1fa1b4","executionInfo":{"status":"ok","timestamp":1680288949303,"user_tz":180,"elapsed":12341475,"user":{"displayName":"Jo達o Gondim","userId":"06361215619043280502"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/90\n","454/454 [==============================] - 311s 595ms/step - loss: 41.0697 - acc: 0.0351 - val_loss: 35.8497 - val_acc: 0.1083\n","Epoch 2/90\n","454/454 [==============================] - 258s 568ms/step - loss: 32.2299 - acc: 0.1199 - val_loss: 28.3558 - val_acc: 0.1708\n","Epoch 3/90\n","454/454 [==============================] - 256s 564ms/step - loss: 26.6515 - acc: 0.1816 - val_loss: 24.8406 - val_acc: 0.2079\n","Epoch 4/90\n","454/454 [==============================] - 258s 567ms/step - loss: 24.1565 - acc: 0.2134 - val_loss: 22.8970 - val_acc: 0.2363\n","Epoch 5/90\n","454/454 [==============================] - 257s 566ms/step - loss: 22.4438 - acc: 0.2427 - val_loss: 21.3489 - val_acc: 0.2660\n","Epoch 6/90\n","454/454 [==============================] - 258s 569ms/step - loss: 21.0966 - acc: 0.2681 - val_loss: 20.1673 - val_acc: 0.2867\n","Epoch 7/90\n","454/454 [==============================] - 256s 564ms/step - loss: 19.9672 - acc: 0.2904 - val_loss: 19.1909 - val_acc: 0.3028\n","Epoch 8/90\n","454/454 [==============================] - 256s 564ms/step - loss: 18.9930 - acc: 0.3065 - val_loss: 18.3420 - val_acc: 0.3171\n","Epoch 9/90\n","454/454 [==============================] - 258s 568ms/step - loss: 18.1703 - acc: 0.3216 - val_loss: 17.6776 - val_acc: 0.3265\n","Epoch 10/90\n","454/454 [==============================] - 257s 565ms/step - loss: 17.4904 - acc: 0.3322 - val_loss: 17.1250 - val_acc: 0.3364\n","Epoch 11/90\n","454/454 [==============================] - 257s 566ms/step - loss: 16.9383 - acc: 0.3416 - val_loss: 16.6933 - val_acc: 0.3410\n","Epoch 12/90\n","454/454 [==============================] - 256s 564ms/step - loss: 16.4838 - acc: 0.3500 - val_loss: 16.3992 - val_acc: 0.3481\n","Epoch 13/90\n","454/454 [==============================] - 258s 567ms/step - loss: 16.0986 - acc: 0.3559 - val_loss: 16.1160 - val_acc: 0.3534\n","Epoch 14/90\n","454/454 [==============================] - 259s 570ms/step - loss: 15.7652 - acc: 0.3641 - val_loss: 15.8474 - val_acc: 0.3593\n","Epoch 15/90\n","454/454 [==============================] - 256s 563ms/step - loss: 15.4757 - acc: 0.3695 - val_loss: 15.6288 - val_acc: 0.3632\n","Epoch 16/90\n","454/454 [==============================] - 258s 569ms/step - loss: 15.2035 - acc: 0.3754 - val_loss: 15.4559 - val_acc: 0.3662\n","Epoch 17/90\n","454/454 [==============================] - 256s 564ms/step - loss: 14.9647 - acc: 0.3802 - val_loss: 15.2087 - val_acc: 0.3710\n","Epoch 18/90\n","454/454 [==============================] - 256s 564ms/step - loss: 14.7456 - acc: 0.3841 - val_loss: 15.1422 - val_acc: 0.3724\n","Epoch 19/90\n","454/454 [==============================] - 256s 564ms/step - loss: 14.5447 - acc: 0.3897 - val_loss: 14.9785 - val_acc: 0.3749\n","Epoch 20/90\n","454/454 [==============================] - 258s 568ms/step - loss: 14.3555 - acc: 0.3925 - val_loss: 14.8481 - val_acc: 0.3757\n","Epoch 21/90\n","454/454 [==============================] - 257s 566ms/step - loss: 14.1760 - acc: 0.3974 - val_loss: 14.7448 - val_acc: 0.3806\n","Epoch 22/90\n","454/454 [==============================] - 256s 564ms/step - loss: 14.0147 - acc: 0.4010 - val_loss: 14.6405 - val_acc: 0.3794\n","Epoch 23/90\n","454/454 [==============================] - 256s 564ms/step - loss: 13.8564 - acc: 0.4040 - val_loss: 14.5800 - val_acc: 0.3838\n","Epoch 24/90\n","454/454 [==============================] - 256s 564ms/step - loss: 13.7071 - acc: 0.4080 - val_loss: 14.4397 - val_acc: 0.3858\n","Epoch 25/90\n","454/454 [==============================] - 256s 564ms/step - loss: 13.5689 - acc: 0.4109 - val_loss: 14.3657 - val_acc: 0.3859\n","Epoch 26/90\n","454/454 [==============================] - 258s 568ms/step - loss: 13.4362 - acc: 0.4149 - val_loss: 14.2809 - val_acc: 0.3902\n","Epoch 27/90\n","454/454 [==============================] - 256s 564ms/step - loss: 13.3075 - acc: 0.4165 - val_loss: 14.2057 - val_acc: 0.3904\n","Epoch 28/90\n","454/454 [==============================] - 256s 564ms/step - loss: 13.1852 - acc: 0.4199 - val_loss: 14.1325 - val_acc: 0.3933\n","Epoch 29/90\n","454/454 [==============================] - 252s 555ms/step - loss: 13.0706 - acc: 0.4218 - val_loss: 14.1406 - val_acc: 0.3927\n","Epoch 30/90\n","454/454 [==============================] - 258s 569ms/step - loss: 12.9615 - acc: 0.4262 - val_loss: 14.0495 - val_acc: 0.3907\n","Epoch 31/90\n","454/454 [==============================] - 256s 564ms/step - loss: 12.8486 - acc: 0.4283 - val_loss: 14.0256 - val_acc: 0.3923\n","Epoch 32/90\n","454/454 [==============================] - 258s 568ms/step - loss: 12.7393 - acc: 0.4305 - val_loss: 13.9903 - val_acc: 0.3957\n","Epoch 33/90\n","454/454 [==============================] - 256s 564ms/step - loss: 12.6406 - acc: 0.4325 - val_loss: 13.9225 - val_acc: 0.3955\n","Epoch 34/90\n","454/454 [==============================] - 256s 564ms/step - loss: 12.5396 - acc: 0.4352 - val_loss: 13.9223 - val_acc: 0.3966\n","Epoch 35/90\n","454/454 [==============================] - 258s 569ms/step - loss: 12.4463 - acc: 0.4376 - val_loss: 13.8778 - val_acc: 0.3975\n","Epoch 36/90\n","454/454 [==============================] - 256s 564ms/step - loss: 12.3525 - acc: 0.4411 - val_loss: 13.8400 - val_acc: 0.3996\n","Epoch 37/90\n","454/454 [==============================] - 257s 566ms/step - loss: 12.2580 - acc: 0.4431 - val_loss: 13.7580 - val_acc: 0.4000\n","Epoch 38/90\n","454/454 [==============================] - 252s 554ms/step - loss: 12.1716 - acc: 0.4447 - val_loss: 13.7957 - val_acc: 0.3968\n","Epoch 39/90\n","454/454 [==============================] - 252s 555ms/step - loss: 12.0824 - acc: 0.4478 - val_loss: 13.7901 - val_acc: 0.3980\n","Epoch 40/90\n","454/454 [==============================] - 258s 569ms/step - loss: 11.9963 - acc: 0.4501 - val_loss: 13.7302 - val_acc: 0.4009\n","Epoch 41/90\n","454/454 [==============================] - 252s 555ms/step - loss: 11.9141 - acc: 0.4517 - val_loss: 13.7875 - val_acc: 0.3961\n","Epoch 42/90\n","454/454 [==============================] - 256s 564ms/step - loss: 11.8289 - acc: 0.4551 - val_loss: 13.7188 - val_acc: 0.3998\n","Epoch 43/90\n","454/454 [==============================] - 256s 564ms/step - loss: 11.7458 - acc: 0.4570 - val_loss: 13.6821 - val_acc: 0.4021\n","Epoch 44/90\n","454/454 [==============================] - 252s 555ms/step - loss: 11.6681 - acc: 0.4589 - val_loss: 13.7664 - val_acc: 0.3978\n","Epoch 45/90\n","454/454 [==============================] - 252s 555ms/step - loss: 11.5858 - acc: 0.4622 - val_loss: 13.6965 - val_acc: 0.4004\n","Epoch 46/90\n","454/454 [==============================] - 252s 555ms/step - loss: 11.5075 - acc: 0.4634 - val_loss: 13.7654 - val_acc: 0.3990\n","Epoch 47/90\n","454/454 [==============================] - 252s 554ms/step - loss: 11.4353 - acc: 0.4654 - val_loss: 13.7224 - val_acc: 0.3970\n","Epoch 48/90\n","454/454 [==============================] - 252s 556ms/step - loss: 11.3578 - acc: 0.4677 - val_loss: 13.6828 - val_acc: 0.4005\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/image_captioning.ipynb","timestamp":1663259576283}],"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}